web-content-folder-item-ids:
  description: "recursively downloads list of entity ids from clearmash web content folders"
  pipeline:
  - run: get_web_content_folder_item_ids
    parameters:
      max-recursion-depth: 2
  - run: dump.to_path
    parameters:
      out-path: ../data/clearmash/web-content-folder-entity-ids


folders:
  description: "get all the folders from the web-content-folder-entity-ids data"
  pipeline:
  - run: load_resource
    parameters:
      url: ../data/clearmash/web-content-folder-entity-ids/datapackage.json
      resource: entity-ids
  - run: filter
    parameters:
      in:
      - is_folder: True
  - run: dump.to_path
    parameters:
      out-path: ../data/clearmash/folders


entity-ids:
  description: "get all the entitiy ids from the web-content-folder-entity-ids data"
  pipeline:
  - run: load_resource
    parameters:
      url: ../data/clearmash/web-content-folder-entity-ids/datapackage.json
      resource: entity-ids
  - run: filter
    parameters:
      out:
      - is_folder: True
  - run: dump.to_path
    parameters:
      out-path: ../data/clearmash/entity-ids


download-entities:
  description: "download entities based on the entity ids from web-content-folder-entitity-ids"
  pipeline:
  - run: load_resource
    parameters:
      url: ../data/clearmash/entity-ids/datapackage.json
      resource: entity-ids
#  - run: filter
#    parameters:
#      in:
#      - id: 115306
#      - id: 115316
#      - id: 232814
#      - id: 232855
#      - id: 4309699
#      - id: 5096050
#      - id: 7436488
#      - id: 115412
#      - id: 115423
#      - id: 115432
#      - id: 115662
#      - id: 115693
#      - id: 115737
#      - id: 116176
#      - id: 116376
#      - id: 3746750
#      - id: 123942
#      - id: 124140
#      - id: 124299
#      - id: 124341
#      - id: 124404
  - run: download_entities
  - run: dump.to_path
    parameters:
      out-path: ../data/clearmash/download-entities

places:
  pipeline:
  - run: load_large_csv_resource
    parameters:
      url: ../data/clearmash/download-entities/datapackage.json
      resource: entities
  - run: prep_entities_for_search
    parameters:
      resource-name: places
      collection-name: places
  - run: dump.to_path
    parameters:
      out-path: ../data/clearmash/places

familynames:
  pipeline:
  - run: load_large_csv_resource
    parameters:
      url: ../data/clearmash/download-entities/datapackage.json
      resource: entities
  - run: prep_entities_for_search
    parameters:
      resource-name: familynames
      collection-name: familyNames
  - run: dump.to_path
    parameters:
      out-path: ../data/clearmash/familynames

movies:
  pipeline:
  - run: load_large_csv_resource
    parameters:
      url: ../data/clearmash/download-entities/datapackage.json
      resource: entities
  - run: prep_entities_for_search
    parameters:
      resource-name: movies
      collection-name: movies
  - run: dump.to_path
    parameters:
      out-path: ../data/clearmash/movies

personalities:
  pipeline:
  - run: load_large_csv_resource
    parameters:
      url: ../data/clearmash/download-entities/datapackage.json
      resource: entities
  - run: prep_entities_for_search
    parameters:
      resource-name: personalities
      collection-name: personalities
  - run: dump.to_path
    parameters:
      out-path: ../data/clearmash/personalities

photounits:
  pipeline:
  - run: load_large_csv_resource
    parameters:
      url: ../data/clearmash/download-entities/datapackage.json
      resource: entities
  - run: prep_entities_for_search
    parameters:
      resource-name: photounits
      collection-name: photoUnits
  - run: dump.to_path
    parameters:
      out-path: ../data/clearmash/photounits


dump_entities_to_elasticsearch:
  pipeline:
  - run: load_large_csv_resource
    parameters:
      url: ../data/clearmash/places/datapackage.json
      resource: places
  - run: load_large_csv_resource
    parameters:
      url: ../data/clearmash/familynames/datapackage.json
      resource: familynames
  - run: load_large_csv_resource
    parameters:
      url: ../data/clearmash/movies/datapackage.json
      resource: movies
  - run: load_large_csv_resource
    parameters:
      url: ../data/clearmash/personalities/datapackage.json
      resource: personalities
  - run: load_large_csv_resource
    parameters:
      url: ../data/clearmash/photounits/datapackage.json
      resource: photounits
  - run: dump_to_es
    parameters:
      indexes:
        mojp:
        - resource-name: places
          doc-type: places
        - resource-name: familynames
          doc-type: familynames
        - resource-name: movies
          doc-type: movies
        - resource-name: personalities
          doc-type: personalities
        - resource-name: photounits
          doc-type: photounits

#download-entities:
#  description: "downloads entities - both new and updated, according to last downloaded and hours_to_next_download, updates db table clearmash-entities"
#  schedule:
#    # daily at 0:15 (presumably after entity-ids were added to DB)
#    crontab: "15 0 * * *"
#  pipeline:
#    - run: ..datapackage_pipelines_mojp.common.processors.load_sql_resource
#      parameters:
#        add-resource: entity-ids
#        schema: (datapackage_pipelines_mojp.clearmash.pipeline_funcs:entity_ids_schema)
#        load-table: clearmash-entity-ids
#        where: (datapackage_pipelines_mojp.clearmash.pipeline_funcs:get_override_item_ids_where)
#    # compares with clearmash-entities table (if exists) to determine if to re-download entities
#    - run: ..datapackage_pipelines_mojp.clearmash.processors.download
#      parameters:
#        input-resource: entity-ids
#        output-resource: entities
#        table: clearmash-entities
#    # deletes entities not allowed for display in ES
#    - run: ..datapackage_pipelines_mojp.common.processors.delete
#      parameters:
#        resource: entities
#        id-field: item_id
#        source: clearmash
#        display-allowed-field: display_allowed
#    - run: ..datapackage_pipelines_mojp.common.processors.dump_to_sql
#      parameters:
#        resource: entities
#        table: clearmash-entities
#
#download-related-entities:
#  description: "downloads related entities for entities in clearmash-entities table, uses logic to reduce number of API calls, but might result in not having all items (e.g. only the first page), updates entity-ids and entities tables in DB"
#  schedule:
#    # daily at 2:30 (after entities were updated in DB)
#    crontab: "30 2 * * *"
#  pipeline:
#    - run: ..datapackage_pipelines_mojp.common.processors.load_sql_resource
#      parameters:
#        add-resource: entities
#        schema: (datapackage_pipelines_mojp.clearmash.pipeline_funcs:entities_schema)
#        load-table: clearmash-entities
#    # gets all the document ids from related fields in the source entry
#    # makes the call to get related fields only when needed
#    # compares against all document ids in the entities table
#    # and also against all document ids downloaded during this pipeline run
#    - run: ..datapackage_pipelines_mojp.clearmash.processors.download_related
#      parameters:
#        input-resource: entities
#        output-resource: related-entities
#        table: clearmash-entities
#    # deletes entities not allowed for display in ES
#    - run: ..datapackage_pipelines_mojp.common.processors.delete
#      parameters:
#        resource: related-entities
#        id-field: item_id
#        source: clearmash
#        display-allowed-field: display_allowed
#    - run: ..datapackage_pipelines_mojp.common.processors.dump_to_sql
#      parameters:
#        resource: related-entities
#        table: clearmash-entities
#
#entities-sync:
#  description: "syncs to elasticsearch all entities from clearmash-entities table for which last_downloaded > last_synced"
#  schedule:
#    # daily at 2:00 and 4:00 (after new and related entities were updated in DB)
#    crontab: "0 2,4 * * *"
#  pipeline:
#    # load new entities (last_synced IS NULL) or recently downloaded entities (last_downloaded > last_synced)
#    - run: ..datapackage_pipelines_mojp.common.processors.load_sql_resource
#      parameters:
#        add-resource: entities
#        schema: (datapackage_pipelines_mojp.clearmash.pipeline_funcs:entities_schema)
#        load-table: clearmash-entities
#        where: (datapackage_pipelines_mojp.clearmash.pipeline_funcs:entities_sync_where)
#    # update the db with last synced time
#    # update is done immediately, as items come in
#    # this ensures items will not be synced even if they fail
#    - run: ..datapackage_pipelines_mojp.common.processors.update_db
#      parameters:
#        resource: entities
#        table: clearmash-entities
#        id-column: item_id
#        id-field: item_id
#        fields:
#          last_synced: (datetime:datetime.now)
#    # filters out items which are not for display
#    # converts to common dbs schema
#    # uses the entities table to get related document ids (doesn't make additional API calls for related docs)
#    - run: ..datapackage_pipelines_mojp.clearmash.processors.convert
#      parameters:
#        input-resource: entities
#        output-resource: dbs-docs
#        related-documents:
#          table: clearmash-entities
#          item-id-column: item_id
#          document-id-column: document_id
#    # sync to elasticsearch
#    - run: ..datapackage_pipelines_mojp.common.processors.sync
#      parameters:
#        input-resource: dbs-docs
#        output-resource: dbs-docs-sync-log
#    # dump sync log to file
#    - run: dump.to_path
#      parameters:
#        out-path: ../data/clearmash/new-entities-sync
#
#entities-delete:
#  description: "go over all items in ElasticSearch and delete any items which are not in entities or have display_allowed=false"
#  schedule:
#    # daily at 06:00 (hopefully after sync is done)
#    crontab: "0 6 * * *"
#  pipeline:
#    # all all dbs docs from elasticsearch matching the given source
#    - run: ..datapackage_pipelines_mojp.common.processors.load_elasticsearch_dbs_docs
#      parameters:
#        source: clearmash
#        add-resource: clearmash-dbs-docs
#    # compares with DB table and only leave rows which should be deleted
#    - run: ..datapackage_pipelines_mojp.clearmash.processors.filter_elastsicsearch_docs_to_delete
#      parameters:
#        input-resource: clearmash-dbs-docs
#        output-resource: clearmash-dbs-docs-to-delete
#        table: clearmash-entities
#    # delete all input docs from elasticsearch
#    - run: ..datapackage_pipelines_mojp.common.processors.delete
#      parameters:
#        resource: clearmash-dbs-docs-to-delete
#        id-field: item_id
#        source: clearmash
#        delete-all-input: true
