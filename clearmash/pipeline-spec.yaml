entity-ids:
  # downloads all the entity ids from the clearmash folders (AKA collections)
  schedule:
    # daily at 00:00
    crontab: "0 0 * * *"
  pipeline:
    - run: ..datapackage_pipelines_mojp.clearmash.processors.add_entity_ids
      parameters:
        add-resource: entity-ids
    - run: dump.to_path
      parameters:
        out-path: ../data/clearmash/entity-ids

entity-ids-update-db:
  # load new entity_ids to the DB
  dependencies:
    # runs in case entity-ids datapackage changed
    - datapackage: data/clearmash/entity-ids/datapackage.json
  pipeline:
    - run: load_resource
      parameters:
        resource: entity-ids
        url: ../data/clearmash/entity-ids/datapackage.json
    # filter out any entity_ids which already exist in the table
    - run: ..datapackage_pipelines_mojp.common.processors.filter_out_existing_ids_in_table
      parameters:
        resource: entity-ids
        table: clearmash-entity-ids
        id-column: item_id
        id-field: item_id
    # appends the new entity ids to DB
    - run: dump.to_sql
      parameters:
        tables:
          clearmash-entity-ids:
            resource-name: entity-ids
            mode: append

download-new-entities:
  # downloads new entities from entity-ids which don't already have a corresponding row in clearmash-entities
  # dumps to path
  schedule:
    # daily at 0:15 (after entity-ids were added to DB)
    crontab: "15 0 * * *"
  pipeline:
    - run: ..datapackage_pipelines_mojp.common.processors.load_sql_resource
      parameters:
        add-resource: entity-ids
        datapackage: ../data/clearmash/entity-ids/datapackage.json
        load-resource: entity-ids
        load-table: clearmash-entity-ids
        where: (datapackage_pipelines_mojp.clearmash.pipeline_funcs:download_new_entities_where)
    - run: ..datapackage_pipelines_mojp.common.processors.filter_out_existing_ids_in_table
      parameters:
        resource: entity-ids
        table: clearmash-entities
        id-column: item_id
        id-field: item_id
    - run: ..datapackage_pipelines_mojp.clearmash.processors.download
      parameters:
        input-resource: entity-ids
        output-resource: entities
    - run: dump.to_path
      parameters:
        out-path: ../data/clearmash/download-new-entities

download-related-entities:
  # downloads related entities
  # uses logic that reduces number of API calls but might result in not having all related items (e.g. only the first page)
  # other pipelines should deal with full syncing of all related items
  schedule:
    # daily at 2:30 (after entities were updated in DB)
    crontab: "30 2 * * *"
  pipeline:
    - run: ..datapackage_pipelines_mojp.common.processors.load_sql_resource
      parameters:
        add-resource: entities
        datapackage: ../data/clearmash/download-new-entities/datapackage.json
        load-resource: entities
        load-table: clearmash-entities
    # gets all the document ids from related fields in the source entry
    # makes the call to get related fields only when needed
    # compares against all document ids in the entities table
    # and also against all document ids downloaded during this pipeline run
    - run: ..datapackage_pipelines_mojp.clearmash.processors.download_related
      parameters:
        input-resource: entities
        output-resource: related-entities
        related-documents:
          table: clearmash-entities
          document-id-column: document_id
          only-download-new: true
    - run: dump.to_path
      parameters:
        out-path: ../data/clearmash/download-related-documents

entities-update-db:
  # load all downloaded entities (new and related) to the clearmash-entities table
  schedule:
    # daily at 1:30 and 3:30 (after new and related entities were downloaded)
    crontab: "30 1,3 * * *"
  pipeline:
    - run: ..datapackage_pipelines_mojp.common.processors.load_resources
      parameters:
        add-resource: entities
        load-resources:
          - resource: entities
            url: ../data/clearmash/download-new-entities/datapackage.json
          - resource: related-entities
            url: ../data/clearmash/download-related-documents/datapackage.json
    # filter out entities which were previously downloaded (base on item_id in entities table)
    - run: ..datapackage_pipelines_mojp.common.processors.filter_out_existing_ids_in_table
      parameters:
        resource: entities
        table: clearmash-entities
        id-column: item_id
        id-field: item_id
    # add some fields to be available in the entities table
    - run: ..datapackage_pipelines_mojp.common.processors.add_fields
      parameters:
        input-resource: entities
        output-resource: entities
        fields:
          - name: last_downloaded
            type: datetime
            value: now
          - name: last_synced
            type: datetime
            value:
          - name: hours_to_next_sync
            type: integer
            value: 0
    - run: dump.to_sql
      parameters:
        tables:
          clearmash-entities:
            resource-name: entities
            mode: append

new-entities-sync:
  # sync the new entities to elasticsearch
  schedule:
    # daily at 2:00 and 4:00 (after new and related entities were updated in DB)
    crontab: "0 2,4 * * *"
  pipeline:
    # load new entities only (hours_to_next_sync=0)
    - run: ..datapackage_pipelines_mojp.common.processors.load_sql_resource
      parameters:
        add-resource: entities
        datapackage: ../data/clearmash/download-new-entities/datapackage.json
        load-resource: entities
        load-table: clearmash-entities
        where: (datapackage_pipelines_mojp.clearmash.pipeline_funcs:new_entities_sync_where)
    # update the db with last synced time, this ensures items will not be synced again as new items (even if they fail)
    - run: ..datapackage_pipelines_mojp.common.processors.update_db
      parameters:
        resource: entities
        table: clearmash-entities
        id-column: item_id
        id-field: item_id
        fields:
          last_synced: (datetime:datetime.now)
          hours_to_next_sync: (datapackage_pipelines_mojp.clearmash.pipeline_funcs:hours_to_next_sync:args)
    # filters out items which are not for display and converts to common dbs schema for syncing to ES
    # uses the entities table to get related document ids (doesn't make additional API calls for related docs)
    - run: ..datapackage_pipelines_mojp.clearmash.processors.convert
      parameters:
        input-resource: entities
        output-resource: dbs-docs
        related-documents:
          table: clearmash-entities
          item-id-column: item_id
          document-id-column: document_id
    # sync to elasticsearch
    - run: ..datapackage_pipelines_mojp.common.processors.sync
      parameters:
        input-resource: dbs-docs
        output-resource: dbs-docs-sync-log
    - run: dump.to_path
      parameters:
        out-path: ../data/clearmash/new-entities-sync

# disable post for now, let's try getting all items first
#post:
#  dependencies:
#    - pipeline: ./clearmash/new-entities-sync
#  pipeline:
#    - run: load_resource
#      parameters:
#        resource: dbs-docs-sync-log
#        url: ../data/clearmash/sync/datapackage.json
#    - run: stream_remote_resources
#    - run: ..datapackage_pipelines_mojp.common.processors.post
#      parameters:
#        resource: dbs-docs-sync-log
#        all_items_query:
#          source: clearmash
#        skip-delete-if:
#          environment:
#            - OVERRIDE_CLEARMASH_COLLECTIONS
#            - OVERRIDE_CLEARMASH_ITEM_IDS
#    - run: dump.to_path
#      parameters:
#        out-path: ../data/clearmash/post
